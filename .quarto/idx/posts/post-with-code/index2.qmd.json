{"title":"Post With Code","markdown":{"yaml":{"title":"Post With Code","author":"Harlow Malloc","date":"2023-11-07","categories":["news","code","analysis"],"image":"image.jpg"},"headingText":"Import Libary","containsRefs":false,"markdown":"\n\nThis is a post with executable code.\n\n```{python}\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\n```\n\n```{python}\nfrom sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier\nimport plotly.graph_objs as go\nimport plotly.offline as py\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, RandomizedSearchCV, RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, roc_auc_score, recall_score, f1_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nimport missingno as msno\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n```\n\n```{python}\n# Download and prepare the data\ndf=pd.read_csv(\"diabetes.csv\")\n\ndf.head()\n\n```\n\n\n```{python}\ndf.info()\n```\n\n\n```{python}\ndf.describe().T\n```\n\n```{python}\n#In this dataset missing data are filled with 0. First, we are gonna change zeros with NaN\n\ndf[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0, np.NaN)\n```\n\nData Visualization\n\nHistogram\nA histogram is a bar graph representation of a grouped data distribution. In other words, it is the transfer of data consisting of repetitive numbers to the table first, and to the chart by using the table, in other words, the graph of the data groups is displayed in rectangular columns.\n\n```{python}\ndf.hist(bins=20,figsize = (15,15));\n```\n\nCountplot and PiePlot\nA count plot can be thought of as a histogram across a categorical, instead of quantitative, variable. A Pie Chart is a type of graph that displays data in a circular graph. The pieces of the graph are proportional to the fraction of the whole in each category.\n\nWe examined distribution of outcome with countplot and pieplot.\n\n```{python}\nimport seaborn as sns\nplt.title(\"Distribution of Outcome\")\nsns.countplot(df[\"Outcome\"], saturation=1)\n```\n\n\n\n```{python}\ndef corr_to_target(dataframe, target, title=None, file=None):\n    plt.figure(figsize=(4,6))\n    sns.heatmap(dataframe.corr()[[target]].sort_values(target,\nascending=False)[1:],annot=True,cmap='coolwarm')\n    plt.title(f'\\n{title}\\n', fontsize=18)\n    plt.show();\n    return\ncorr_to_target(df, \"Outcome\", title=\"Outcome\")\n```\n\nCorrelation matrix of variables with each other.\n\n```{python}\ncorr_matrix = df.corr()\nsns.clustermap(corr_matrix, annot=True, fmt=\".2f\")\nplt.title(\"Correlation Between Features\")\n```\n\n\nSplitting Train and Test Set\nAbove, we first gave all variables except the \"outcome\" variable to the X variable and gave the variable \"outcome\" to the y variable. Then we split the data into train and test data. X_train and y_train show the dependent and independent variables to be used to test the model, while X_test and y_test are used to develop the model. Test_size specifies how many% of data (30%) will be used for testing. Random_state is used to see the same distinction every time we run the program. Stratify provides a balanced separation of classes in the y variable when separating.\n\n```{python}\n#y = df[\"Outcome\"]\n#X = df.drop([\"Outcome\"], axis = 1)\ntrain,test = train_test_split(df, test_size=0.3, random_state = 2)\n```\n\n```{python}\ntrain.isnull().sum()\n```\n\n```{python}\ntest.isnull().sum()\n```\n\n\nHandling with Missing Values\nAfter filling the 0s with the value of NaN, the missing values ​​will be visualized. We use the missingno library for this.\n\n```{python}\nmsno.bar(df,figsize=(10,6))\n```\n\nWe will fill in each missing value with its median value.\n\n```{python}\ndef median_target(dataf, var):   \n    temp = dataf[dataf[var].notnull()]\n    temp = temp[[var, 'Outcome']].groupby(['Outcome'])[[var]].median().reset_index()\n    return temp\n```\n\n```{python}\ncolumns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n```\n\nAfter filling if we examine null values in dataset, we will see there are not any missing values.\n\n\n\n```{python}\nprint(\"TRAIN DATA\")\nprint(train.isnull().sum(), \"\\n\")\nprint(\"TEST DATA\")\nprint(test.isnull().sum())\n```\n\nPlotting Roc Curve\nROC curves are frequently used to show in a graphical way the connection/trade-off between clinical sensitivity and specificity for every possible cut-off for a test or a combination of tests. In addition the area under the ROC curve gives an idea about the benefit of using the test(s) in question.\n\n```{python}\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0,1],[0,1],\"k--\")\n    plt.axis([0,1,0,1])\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n```\n\nMachine Learning\nWe will use 6 different machine learning algorithm for this model and examine ROC score, accuracy test and train score, best parameters and ROC curve\n\n```{python}\nX_train = train.iloc[:,:8]\ny_train = train.iloc[:,-1:]\n\nX_test = test.iloc[:,:8]\ny_test = test.iloc[:,-1:]\n```\n\n```{python}\ndef ml_model(model, parameters):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n    random_search = RandomizedSearchCV(model, parameters, cv=cv, random_state=1, n_jobs=-1, verbose=1)\n    pipe = make_pipeline(StandardScaler(),random_search)\n    pipe.fit(X_train, y_train)\n    y_pred_proba = pipe.predict_proba(X_test)[:,1]\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n    print(\"ROC Score : \",roc_auc_score(y_test, y_pred_proba))\n    print(\"F1 score for train: \", f1_score(y_train, pipe.predict(X_train)))\n    print(\"F1 score for test: \" , f1_score(y_test, pipe.predict(X_test)))\n    print(\"Best params:\" + str(random_search.best_params_))\n    plot_roc_curve(fpr, tpr)\n    \nlog_reg_params = {\"C\" : [1,2,3,0.01,0.001, 2.5, 1.5],\n                  \"max_iter\" : range(100,800,100)}\nknn_params = {\"n_neighbors\" : np.arange(1,50),\n              \"leaf_size\" : np.arange(1,50)}\ndecTree_params = {\"max_depth\" : [5,10,15,20,25,30],\n                  \"min_samples_split\" : np.arange(2,50),\n                  \"min_samples_leaf\" : np.arange(1,50)}\nrandomForest_params = {\"n_estimators\" : [100,500, 1000],\n                       \"min_samples_split\" : np.arange(2,30),\n                       \"min_samples_leaf\" : np.arange(1,50),\n                       \"max_features\" : np.arange(1,7)}\nlgbm_params = {\"n_estimators\" : [100,500,1000],\n               \"subsample\" : [0.6,0.8,1.0],\n               \"max_depth\" : [5,10,15,20,25,30],\n               \"learning_rate\" : [0.1, 0.01, 0.02, 0.5],\n               \"min_child_samples\" : np.arange(2,30)}\n\nsgd_params = {\"alpha\" : [0.0001, 0.1, 0.001, 0.01],\n              \"max_iter\" : [100,500,1000,2000],\n              \"loss\" : [\"log\",\"modified_huber\",\"perceptron\"]}\n\n```\n\n```{python}\nml_model(LogisticRegression(), log_reg_params)\n```\n\n```{python}\nml_model(KNeighborsClassifier(), knn_params)\n```","srcMarkdownNoYaml":"\n\nThis is a post with executable code.\n\n```{python}\n# Import Libary\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\n```\n\n```{python}\nfrom sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier\nimport plotly.graph_objs as go\nimport plotly.offline as py\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, RandomizedSearchCV, RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, roc_auc_score, recall_score, f1_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nimport missingno as msno\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n```\n\n```{python}\n# Download and prepare the data\ndf=pd.read_csv(\"diabetes.csv\")\n\ndf.head()\n\n```\n\n\n```{python}\ndf.info()\n```\n\n\n```{python}\ndf.describe().T\n```\n\n```{python}\n#In this dataset missing data are filled with 0. First, we are gonna change zeros with NaN\n\ndf[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0, np.NaN)\n```\n\nData Visualization\n\nHistogram\nA histogram is a bar graph representation of a grouped data distribution. In other words, it is the transfer of data consisting of repetitive numbers to the table first, and to the chart by using the table, in other words, the graph of the data groups is displayed in rectangular columns.\n\n```{python}\ndf.hist(bins=20,figsize = (15,15));\n```\n\nCountplot and PiePlot\nA count plot can be thought of as a histogram across a categorical, instead of quantitative, variable. A Pie Chart is a type of graph that displays data in a circular graph. The pieces of the graph are proportional to the fraction of the whole in each category.\n\nWe examined distribution of outcome with countplot and pieplot.\n\n```{python}\nimport seaborn as sns\nplt.title(\"Distribution of Outcome\")\nsns.countplot(df[\"Outcome\"], saturation=1)\n```\n\n\n\n```{python}\ndef corr_to_target(dataframe, target, title=None, file=None):\n    plt.figure(figsize=(4,6))\n    sns.heatmap(dataframe.corr()[[target]].sort_values(target,\nascending=False)[1:],annot=True,cmap='coolwarm')\n    plt.title(f'\\n{title}\\n', fontsize=18)\n    plt.show();\n    return\ncorr_to_target(df, \"Outcome\", title=\"Outcome\")\n```\n\nCorrelation matrix of variables with each other.\n\n```{python}\ncorr_matrix = df.corr()\nsns.clustermap(corr_matrix, annot=True, fmt=\".2f\")\nplt.title(\"Correlation Between Features\")\n```\n\n\nSplitting Train and Test Set\nAbove, we first gave all variables except the \"outcome\" variable to the X variable and gave the variable \"outcome\" to the y variable. Then we split the data into train and test data. X_train and y_train show the dependent and independent variables to be used to test the model, while X_test and y_test are used to develop the model. Test_size specifies how many% of data (30%) will be used for testing. Random_state is used to see the same distinction every time we run the program. Stratify provides a balanced separation of classes in the y variable when separating.\n\n```{python}\n#y = df[\"Outcome\"]\n#X = df.drop([\"Outcome\"], axis = 1)\ntrain,test = train_test_split(df, test_size=0.3, random_state = 2)\n```\n\n```{python}\ntrain.isnull().sum()\n```\n\n```{python}\ntest.isnull().sum()\n```\n\n\nHandling with Missing Values\nAfter filling the 0s with the value of NaN, the missing values ​​will be visualized. We use the missingno library for this.\n\n```{python}\nmsno.bar(df,figsize=(10,6))\n```\n\nWe will fill in each missing value with its median value.\n\n```{python}\ndef median_target(dataf, var):   \n    temp = dataf[dataf[var].notnull()]\n    temp = temp[[var, 'Outcome']].groupby(['Outcome'])[[var]].median().reset_index()\n    return temp\n```\n\n```{python}\ncolumns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n```\n\nAfter filling if we examine null values in dataset, we will see there are not any missing values.\n\n\n\n```{python}\nprint(\"TRAIN DATA\")\nprint(train.isnull().sum(), \"\\n\")\nprint(\"TEST DATA\")\nprint(test.isnull().sum())\n```\n\nPlotting Roc Curve\nROC curves are frequently used to show in a graphical way the connection/trade-off between clinical sensitivity and specificity for every possible cut-off for a test or a combination of tests. In addition the area under the ROC curve gives an idea about the benefit of using the test(s) in question.\n\n```{python}\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0,1],[0,1],\"k--\")\n    plt.axis([0,1,0,1])\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n```\n\nMachine Learning\nWe will use 6 different machine learning algorithm for this model and examine ROC score, accuracy test and train score, best parameters and ROC curve\n\n```{python}\nX_train = train.iloc[:,:8]\ny_train = train.iloc[:,-1:]\n\nX_test = test.iloc[:,:8]\ny_test = test.iloc[:,-1:]\n```\n\n```{python}\ndef ml_model(model, parameters):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n    random_search = RandomizedSearchCV(model, parameters, cv=cv, random_state=1, n_jobs=-1, verbose=1)\n    pipe = make_pipeline(StandardScaler(),random_search)\n    pipe.fit(X_train, y_train)\n    y_pred_proba = pipe.predict_proba(X_test)[:,1]\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n    print(\"ROC Score : \",roc_auc_score(y_test, y_pred_proba))\n    print(\"F1 score for train: \", f1_score(y_train, pipe.predict(X_train)))\n    print(\"F1 score for test: \" , f1_score(y_test, pipe.predict(X_test)))\n    print(\"Best params:\" + str(random_search.best_params_))\n    plot_roc_curve(fpr, tpr)\n    \nlog_reg_params = {\"C\" : [1,2,3,0.01,0.001, 2.5, 1.5],\n                  \"max_iter\" : range(100,800,100)}\nknn_params = {\"n_neighbors\" : np.arange(1,50),\n              \"leaf_size\" : np.arange(1,50)}\ndecTree_params = {\"max_depth\" : [5,10,15,20,25,30],\n                  \"min_samples_split\" : np.arange(2,50),\n                  \"min_samples_leaf\" : np.arange(1,50)}\nrandomForest_params = {\"n_estimators\" : [100,500, 1000],\n                       \"min_samples_split\" : np.arange(2,30),\n                       \"min_samples_leaf\" : np.arange(1,50),\n                       \"max_features\" : np.arange(1,7)}\nlgbm_params = {\"n_estimators\" : [100,500,1000],\n               \"subsample\" : [0.6,0.8,1.0],\n               \"max_depth\" : [5,10,15,20,25,30],\n               \"learning_rate\" : [0.1, 0.01, 0.02, 0.5],\n               \"min_child_samples\" : np.arange(2,30)}\n\nsgd_params = {\"alpha\" : [0.0001, 0.1, 0.001, 0.01],\n              \"max_iter\" : [100,500,1000,2000],\n              \"loss\" : [\"log\",\"modified_huber\",\"perceptron\"]}\n\n```\n\n```{python}\nml_model(LogisticRegression(), log_reg_params)\n```\n\n```{python}\nml_model(KNeighborsClassifier(), knn_params)\n```"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index2.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"cosmo","title-block-banner":true,"title":"Post With Code","author":"Harlow Malloc","date":"2023-11-07","categories":["news","code","analysis"],"image":"image.jpg"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}