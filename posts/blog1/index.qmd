---
title: "Linear Regression  and Non Linear Regression"
author: "Zeel"
date: "2023-11-04"
categories: [Machine Learning]



---

This is my 1st blog. Welcome!

![](thumbnail.jpg)

Since this post doesn't specify an explicit `image`, the first image in the post will be used in the listing page of posts.

Method 1:

```{python}
# Import Libary
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Download and prepare the data
df=pd.read_csv("headbrain.csv")

df.head()

```

```{python}
# Import Libary
print(df.isnull().sum())

	
# Declare dependent variable(Y) and independent variable(X)

X=df['Head Size(cm^3)'].values
Y = df['Brain Weight(grams)'].values

```

```{python}
df.info()

```

```{python}

df.shape
```

```{python}
X.shape
```


```{python}

Y.shape

```

```{python}
np.corrcoef(X, Y)

```


```{python}
# Plot the Input Data
plt.scatter(X, Y, c='green', label='Data points')
plt.xlabel('Head Size in cm3')
plt.ylabel('Brain Weight in grams')
plt.legend()
plt.show()
```



```{python}
# Plot the Input Data
# Calculating coefficient

# Mean X and Y
mean_x = np.mean(X)
mean_y = np.mean(Y)

# Total number of values
n = len(X)

# Using the formula to calculate theta1 and theta2
numer = 0
denom = 0
for i in range(n):
    numer += (X[i] - mean_x) * (Y[i] - mean_y)
    denom += (X[i] - mean_x) ** 2
b1 = numer / denom
b0 = mean_y - (b1 * mean_x)

# Printing coefficients
print("coefficients for regression",b1, b0)
```

```{python}
# Plotting Values and Regression Line
%matplotlib inline

plt.rcParams['figure.figsize'] = (10.0, 5.0)
# max_x = np.max(X) + 100
# min_x = np.min(X) - 100

y = b0 + b1 * X

# Ploting Line
plt.plot(X, y, color='blue', label='Regression Line')
# Ploting Scatter Points
plt.scatter(X, Y, c='green', label='Scatter data')

plt.xlabel('Head Size in cm3')
plt.ylabel('Brain Weight in grams')
plt.legend()
plt.show()
```

```{python}
# Calculating Root Mean Squares Error
rmse = 0
for i in range(n):
    y_pred = b0 + b1 * X[i]
    rmse += (Y[i] - y_pred) ** 2
    
rmse = np.sqrt(rmse/n)
print("Root Mean Square Error is",rmse)
```

```{python}
# Calculating R2 Score
ss_tot = 0
ss_res = 0
for i in range(n):
    y_pred = b0 + b1 * X[i]
    ss_tot += (Y[i] - mean_y) ** 2
    ss_res += (Y[i] - y_pred) ** 2
r2 = 1 - (ss_res/ss_tot)
print("R2 Score",r2)
```


Method 2:

```{python}
# Import necessary libraries
import pandas as pd

# Load the dataset
df = pd.read_csv('headbrain.csv')

# Explore the dataset
df.head()
```

```{python}
print(data.isnull().sum())

```



```{python}
mean_x = np.mean(X)
mean_y = np.mean(Y)

#Total number of Values
n = len(X)

```

```{python}
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

X = X.reshape((n,1))

```

```{python}
model = LinearRegression()
```

```{python}
model = model.fit(X,Y)
```

```{python}
r2 = model.score(X,Y)
print('R^2 value: ',r2)
```

```{python}
Y_pred = model.predict(X)

```

Method 2b

```{python}
from sklearn.model_selection import train_test_split

X = df[['Head Size(cm^3)']]  # Select relevant features
y = df['Brain Weight(grams)']  # Define the target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

```{python}
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score

model = LinearRegression() # Create a linear regression model

model.fit(X_train, y_train) # Fit the model to the training data

```

```{python}
y_pred = model.predict(X_test)
```

```{python}

plt.scatter(y_test, y_pred) # Scatter plot to visualize actual vs. predicted values

plt.xlabel("Actual Sale Prices")
plt.ylabel("Predicted Sale Prices")
plt.title("Actual Sale Prices vs. Predicted Sale Prices")
plt.show()
```

```{python}
# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)

# Calculate R-squared (R²) score
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"R-squared (R²) Score: {r2:.2f}")
```



2. Non-Linear Regression

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Read in our Data Set
df = pd.read_csv("headbrain.csv")
```

```{python}

X=df['Head Size(cm^3)'].values
Y = df['Brain Weight(grams)'].values
```


```{python}
from sklearn.preprocessing import PolynomialFeatures
polynomial_converter = PolynomialFeatures(degree=2,include_bias=False)
# Converter "fits" to data, in this case, reads in every X column
# Then it "transforms" and ouputs the new polynomial data
poly_features = polynomial_converter.fit_transform(X)
poly_features.shape
```

```{python}
from sklearn.model_selection import train_test_split
# random_state:
# https://stackoverflow.com/questions/28064634/random-state-pseudo-random-number-in-scikit-learn
X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=101)
```

```{python}
from sklearn.linear_model import LinearRegression
model = LinearRegression(fit_intercept=True)
model.fit(X_train,y_train)
```

```{python}
test_predictions = model.predict(X_test)
from sklearn.metrics import mean_absolute_error,mean_squared_error
MAE = mean_absolute_error(y_test,test_predictions)
MSE = mean_squared_error(y_test,test_predictions)
RMSE = np.sqrt(MSE)
```

```{python}
# TRAINING ERROR PER DEGREE
train_rmse_errors = []
# TEST ERROR PER DEGREE
test_rmse_errors = []

for d in range(1,10):

    # CREATE POLY DATA SET FOR DEGREE "d"
    polynomial_converter = PolynomialFeatures(degree=d,include_bias=False)
    poly_features = polynomial_converter.fit_transform(X)

    # SPLIT THIS NEW POLY DATA SET
    X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=101)

    # TRAIN ON THIS NEW POLY SET
    model = LinearRegression(fit_intercept=True)
    model.fit(X_train,y_train)

    # PREDICT ON BOTH TRAIN AND TEST
    train_pred = model.predict(X_train)
    test_pred = model.predict(X_test)

    # Calculate Errors

    # Errors on Train Set
    train_RMSE = np.sqrt(mean_squared_error(y_train,train_pred))

    # Errors on Test Set
    test_RMSE = np.sqrt(mean_squared_error(y_test,test_pred))

    # Append errors to lists for plotting later


    train_rmse_errors.append(train_RMSE)
    test_rmse_errors.append(test_RMSE)
```

```{python}
plt.plot(range(1,6),train_rmse_errors[:5],label='TRAIN')
plt.plot(range(1,6),test_rmse_errors[:5],label='TEST')
plt.xlabel("Polynomial Complexity")
plt.ylabel("RMSE")
plt.legend()
```

```{python}
# Based on our chart, could have also been degree=4, but
# it is better to be on the safe side of complexity
final_poly_converter = PolynomialFeatures(degree=3,include_bias=False)
final_model = LinearRegression()
final_model.fit(final_poly_converter.fit_transform(X),y)

from joblib import dump, load
dump(final_model, 'sales_poly_model.joblib')
dump(final_poly_converter,'poly_converter.joblib')


loaded_poly = load('poly_converter.joblib')
loaded_model = load('sales_poly_model.joblib')
campaign = [[149,22,12]]
campaign_poly = loaded_poly.transform(campaign)
campaign_poly

final_model.predict(campaign_poly)
```

```{python}

```

```{python}

```


Check with TA

```{python}
import matplotlib.pyplot as plt
import numpy as np

#define predictor and response variables
x = np.array([2, 3, 4, 5, 6, 7, 7, 8, 9, 11, 12])
y = np.array([18, 16, 15, 17, 20, 23, 25, 28, 31, 30, 29])

#create scatterplot to visualize relationship between x and y
plt.scatter(x, y)
```

```{python}
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

#specify degree of 3 for polynomial regression model
#include bias=False means don't force y-intercept to equal zero
poly = PolynomialFeatures(degree=3, include_bias=False)

#reshape data to work properly with sklearn
poly_features = poly.fit_transform(x.reshape(-1, 1))

#fit polynomial regression model
poly_reg_model = LinearRegression()
poly_reg_model.fit(poly_features, y)

#display model coefficients
print(poly_reg_model.intercept_, poly_reg_model.coef_)
```

```{python}
#use model to make predictions on response variable
y_predicted = poly_reg_model.predict(poly_features)

#create scatterplot of x vs. y
plt.scatter(x, y)

#add line to show fitted polynomial regression model
plt.plot(x, y_predicted, color='purple')
```


Try non-linear on headbrain

```{python}
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
#from sklearn.linear_model import LinearRegression

# Download and prepare the data
df=pd.read_csv("headbrain.csv")

df.head()
```

```{python}

X=df['Head Size(cm^3)'].values
Y = df['Brain Weight(grams)'].values
```

```{python}


# Fitting Polynomial Regression to the dataset
from sklearn.preprocessing import PolynomialFeatures
X=X.reshape(-1,1)
Y=Y.reshape(-1,1)
poly = PolynomialFeatures(degree=4)
X_poly = poly.fit_transform(X)
 
poly.fit(X_poly, Y)
lin2 = LinearRegression()


lin2.fit(X_poly, Y)
```

```{python}



# Visualising the Polynomial Regression results
plt.scatter(X, Y, color='blue')
 
plt.plot(X, lin2.predict(poly.fit_transform(X)),
         color='red')
plt.title('Polynomial Regression')
plt.xlabel('Head')
plt.ylabel('Brain')
 
plt.show()
```

```{python}

X=df['Head Size(cm^3)'].values
Y = df['Brain Weight(grams)'].values
```

```{python}

X=df['Head Size(cm^3)'].values
Y = df['Brain Weight(grams)'].values
```